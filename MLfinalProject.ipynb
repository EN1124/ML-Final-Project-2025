{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15599f1b",
   "metadata": {},
   "source": [
    "## **Loan Default Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84019c9",
   "metadata": {},
   "source": [
    "### **Goal:**\n",
    "#### -Identify if a borrower is at risk of failing to repay their loan\n",
    "\n",
    "### **Process:**\n",
    "#### -Use applicant data to develop a model that predicts if a borrower will default on their loan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df3067b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics needed to determine if a borrower is at risk of default \n",
    "\n",
    "#Quantitative metrics\n",
    "#1. PD probability of default -> found using Machine learning \n",
    "#2. credit score -> located in data set\n",
    "#3. Loss Given Default (LGD) and Exposure at Default (EAD): (N/A for this project)\n",
    "\n",
    "#Qualitative metrics\n",
    "#1. income stability, employment history, DTI ratio (debt to income ratio = total monthly debt/total monthly income *100)\n",
    "#2. Loan properties (size of loan, term, interest rate, collateral assets) \n",
    "#3. Management quality (for businesses), history of management team   (N/A for this project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc8893",
   "metadata": {},
   "source": [
    "#### **1. Data Loading and Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "079f5bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchinfo \n",
    "import seaborn as sns\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3906642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoanID            0\n",
      "Age               0\n",
      "Income            0\n",
      "LoanAmount        0\n",
      "CreditScore       0\n",
      "MonthsEmployed    0\n",
      "NumCreditLines    0\n",
      "InterestRate      0\n",
      "LoanTerm          0\n",
      "DTIRatio          0\n",
      "Education         0\n",
      "EmploymentType    0\n",
      "MaritalStatus     0\n",
      "HasMortgage       0\n",
      "HasDependents     0\n",
      "LoanPurpose       0\n",
      "HasCoSigner       0\n",
      "Default           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Load data into pandas dataframe\n",
    "df = pd.read_csv(\"Loan_default.csv\")\n",
    "\n",
    "#check and drop any rows that contain empty values      *(for this exercise the data is already clean)\n",
    "print(df.isnull().sum())\n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "#drop loanID as it will not be used in the analysis\n",
    "df = df.drop(['LoanID'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a2549e",
   "metadata": {},
   "source": [
    "#### **2. Data Manipulation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c741e21",
   "metadata": {},
   "source": [
    "#### **2.1 Baseline (majority class) accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cda7c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default\n",
      "0          225694\n",
      "1           29653\n",
      "Name: count, dtype: int64\n",
      "0.8838717509898295\n"
     ]
    }
   ],
   "source": [
    "#calculating the most frequent class   (0 or 1)\n",
    "#final model performance must be above the baseline to indicate accurate predictions\n",
    "default_count = pd.DataFrame(df['Default']).value_counts()\n",
    "print(default_count)\n",
    "\n",
    "a = 225694+29653\n",
    "b = 225694/a\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d9520",
   "metadata": {},
   "source": [
    "### The dataset contains about 88% of the non-default class, therefore we will add downsampling and upweighting to handle this imbalanced dataset. This makes sure the model does not just choose the most common class when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "564e1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-Balanced dataset: 59306 total sample\n",
      "Default\n",
      "0          29653\n",
      "1          29653\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#get the indices for each class, remove samples at random indices from the majority class,combine into new dataframe\n",
    "majority_index = df[df['Default'] == 0].index\n",
    "minority_index = df[df['Default'] == 1].index\n",
    "\n",
    "# Downsample majority to minority size\n",
    "downsample_size = len(minority_index)\n",
    "downsample_index = np.random.choice(majority_index, size=downsample_size, replace=False)\n",
    "\n",
    "#Combined new indices\n",
    "balanced_index = np.concatenate([downsample_index, minority_index])\n",
    "df_rebalanced = df.loc[balanced_index].reset_index(drop=True)\n",
    "\n",
    "print(f\"Re-Balanced dataset: {len(df_rebalanced)} total sample\")\n",
    "print(pd.DataFrame(df_rebalanced['Default']).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7093445",
   "metadata": {},
   "source": [
    "### **2.2 Dataset Contents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9218de94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n18 total columns (original)\\n\\nindependent variables (features):\\n9 numeric variables                 (excluding binary label 0,1 for default result)\\n4 relevant non-numeric variables    (excluding loanID)\\n3 boolean variables                 (yes, no  values, convert to 1,0)\\n\\ndependent variable (target):\\n1 boolean column values to label as default or not\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "18 total columns (original)\n",
    "\n",
    "independent variables (features):\n",
    "9 numeric variables                 (excluding binary label 0,1 for default result)\n",
    "4 relevant non-numeric variables    (excluding loanID)\n",
    "3 boolean variables                 (yes, no  values, convert to 1,0)\n",
    "\n",
    "dependent variable (target):\n",
    "1 boolean column values to label as default or not\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e37051d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education: [\"Bachelor's\" \"Master's\" 'High School' 'PhD']\n",
      "employment: ['Full-time' 'Unemployed' 'Self-employed' 'Part-time']\n",
      "marital status: ['Divorced' 'Married' 'Single']\n",
      "loan purpose: ['Other' 'Auto' 'Business' 'Home' 'Education']\n",
      "min credit score: 849\n",
      "months employed min and max: 0, 119\n",
      "num credit lines min and max: 1, 4\n",
      "InterestRate min and max: 2.0, 25.0\n",
      "LoanTerm min and max: 12, 60\n",
      "DTI Ratio min and max: 0.1, 0.9\n"
     ]
    }
   ],
   "source": [
    "#get unique values (for non-numeric variables)\n",
    "unique_education = df['Education'].unique()\n",
    "\n",
    "unique_employment = df['EmploymentType'].unique()\n",
    "\n",
    "unique_marital = df['MaritalStatus'].unique()\n",
    "\n",
    "unique_purpose = df['LoanPurpose'].unique()\n",
    "\n",
    "print(f\"education: {unique_education}\\nemployment: {unique_employment}\\nmarital status: {unique_marital}\\nloan purpose: {unique_purpose}\")\n",
    "\n",
    "#get min and max values to estimate a range for each column in the training/testing data\n",
    "minCreditscore = df['CreditScore'].max()\n",
    "print(f\"min credit score: {minCreditscore}\")\n",
    "\n",
    "monthsEmployed_min = df['MonthsEmployed'].min()\n",
    "monthsEmployed_max = df['MonthsEmployed'].max()\n",
    "print(f\"months employed min and max: {monthsEmployed_min}, {monthsEmployed_max}\")\n",
    "\n",
    "numCreditLines_min = df['NumCreditLines'].min()\n",
    "numCreditLines_max = df['NumCreditLines'].max()\n",
    "print(f\"num credit lines min and max: {numCreditLines_min}, {numCreditLines_max}\")\n",
    "\n",
    "interestRate_min = df['InterestRate'].min()\n",
    "interestRate_max = df['InterestRate'].max()\n",
    "print(f\"InterestRate min and max: {interestRate_min}, {interestRate_max}\")\n",
    "\n",
    "loanTerm_min = df['LoanTerm'].min()\n",
    "loanTerm_max = df['LoanTerm'].max()\n",
    "print(f\"LoanTerm min and max: {loanTerm_min}, {loanTerm_max}\")\n",
    "\n",
    "DTIRatio_min = df['DTIRatio'].min()\n",
    "DTIRatio_max = df['DTIRatio'].max()\n",
    "print(f\"DTI Ratio min and max: {DTIRatio_min}, {DTIRatio_max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a3f5a",
   "metadata": {},
   "source": [
    "#### **2.2 Training and Testing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75be3dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader,random_split\n",
    "import pytorch_lightning as L\n",
    "import ModelsFinal as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc6bb87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Testing data\n",
    "\n",
    "df_train_test = pd.DataFrame(df_rebalanced)\n",
    "#display(df_train_test)\n",
    "\n",
    "#use one hot encoding to convert the non-numeric categorical values and boolean values, education,employment,marital status,loan purpose\n",
    "#set drop_first=True to drop the first dummy column to avoid multicollinearity, i.e avoid the indepenedent variables being affected by each other\n",
    "df_encoded = pd.get_dummies(df_train_test, columns=['Education','EmploymentType','MaritalStatus','LoanPurpose','HasMortgage','HasDependents','HasCoSigner'],drop_first=True, dtype=float)\n",
    "#display(df_train_test_encoded)\n",
    "\n",
    "#define independent and dependent variables, (x,y) \n",
    "#drop LoanID, put into new dataframe\n",
    "#instead of typing each individual category just assign the encoded dataframe to x but drop the target category (depenedent variable Default)\n",
    "x_df = df_encoded.drop(['Default'],axis=1)\n",
    "y_df = df_encoded['Default']\n",
    "\n",
    "#convert to torch tensor format\n",
    "x_full = torch.tensor(x_df.to_numpy(),dtype=torch.float32)\n",
    "y_full = torch.tensor(y_df.to_numpy(),dtype=torch.float32)\n",
    "\n",
    "#train test split (80% train, 20% test)\n",
    "dataset_full = TensorDataset(x_full, y_full)\n",
    "\n",
    "dataset_size = len(dataset_full)\n",
    "train_split: int = int(0.8*dataset_size)\n",
    "test_split: int = dataset_size-train_split\n",
    "\n",
    "#set seed to 25 for pytorch rng consistency\n",
    "torch.manual_seed(25)\n",
    "train_set, test_set = random_split(dataset_full, [train_split, test_split])\n",
    "\n",
    "#Final DataLoaders (batch size of 32 is ideal)\n",
    "train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470fe383",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression (Linear Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dba3b2",
   "metadata": {},
   "source": [
    "### The basic Logistic regression model modeled using a neural network with 1 layer and a sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c17d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "LinearModel                              --\n",
       "â”œâ”€BinaryAccuracy: 1-1                    --\n",
       "â”œâ”€Sequential: 1-2                        --\n",
       "â”‚    â””â”€Linear: 2-1                       25\n",
       "=================================================================\n",
       "Total params: 25\n",
       "Trainable params: 25\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Linear model Train (total feautures = 24 after encoding)\n",
    "model_LR = models.LinearModel(x_df.shape[1])\n",
    "torchinfo.summary(model_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "011939ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "c:\\Users\\Master\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "c:\\Users\\Master\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name     | Type           | Params | Mode  | FLOPs\n",
      "------------------------------------------------------------\n",
      "0 | accuracy | BinaryAccuracy | 0      | train | 0    \n",
      "1 | model    | Sequential     | 25     | train | 0    \n",
      "------------------------------------------------------------\n",
      "25        Trainable params\n",
      "0         Non-trainable params\n",
      "25        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n",
      "c:\\Users\\Master\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearModel(\n",
      "  (accuracy): BinaryAccuracy()\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1483/1483 [00:17<00:00, 84.55it/s, v_num=35, train_step_acc=0.350]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1483/1483 [00:17<00:00, 84.39it/s, v_num=35, train_step_acc=0.350]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Master\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:434: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 371/371 [00:03<00:00, 122.75it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      test_step_acc         0.5092732906341553\n",
      "     test_step_loss          8.999670028686523\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_step_acc': 0.5092732906341553, 'test_step_loss': 8.999670028686523}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(model_LR)\n",
    "trainer = L.Trainer(max_epochs=3)\n",
    "trainer.fit(model_LR,train_dataloaders=train_dataloader)\n",
    "trainer.test(model_LR,dataloaders=test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5d21ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Metrics: {'test_step_acc': tensor(0.5093), 'test_step_loss': tensor(8.9997)}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final Metrics: {trainer.logged_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97dc3a6",
   "metadata": {},
   "source": [
    "### An accuracy score around 50% shows a basic logistic regression model is not sufficient enough to capture the details of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322fc7a7",
   "metadata": {},
   "source": [
    "## **4. Multi Layer Perceptron (MLP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4e91c",
   "metadata": {},
   "source": [
    "An MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb1bd245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer_mlp = L.Trainer(max_epochs=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c0f1bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MLP                                      --\n",
       "â”œâ”€BinaryAccuracy: 1-1                    --\n",
       "â”œâ”€Sequential: 1-2                        --\n",
       "â”‚    â””â”€Linear: 2-1                       2,500\n",
       "â”‚    â””â”€ReLU: 2-2                         --\n",
       "â”‚    â””â”€Linear: 2-3                       101\n",
       "=================================================================\n",
       "Total params: 2,601\n",
       "Trainable params: 2,601\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_MLP = models.MLP(x_df.shape[1])\n",
    "torchinfo.summary(model_MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b937da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type           | Params | Mode  | FLOPs\n",
      "------------------------------------------------------------\n",
      "0 | accuracy | BinaryAccuracy | 0      | train | 0    \n",
      "1 | model    | Sequential     | 2.6 K  | train | 0    \n",
      "------------------------------------------------------------\n",
      "2.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1483/1483 [00:19<00:00, 77.20it/s, v_num=36, train_step_acc=0.550] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=34` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1483/1483 [00:19<00:00, 76.88it/s, v_num=36, train_step_acc=0.550]\n",
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 371/371 [00:03<00:00, 121.90it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      test_step_acc         0.5228460431098938\n",
      "     test_step_loss          5.409454345703125\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_step_acc': 0.5228460431098938, 'test_step_loss': 5.409454345703125}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_mlp.fit(model_MLP,train_dataloaders=train_dataloader)\n",
    "trainer_mlp.test(model_MLP,dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2904340b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Metrics: {'test_step_acc': tensor(0.5219), 'test_step_loss': tensor(4.9107)}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final Metrics: {trainer_mlp.logged_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f91ae2",
   "metadata": {},
   "source": [
    "## **5. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11327a52",
   "metadata": {},
   "source": [
    "### Both models Showed less than ideal performance during this experiment <80% accuracy, however the MLP did slightly outperform the logistic regression model showing it was able to capture more detailed relationships between the input features, and adjust the weights and biases for a overall better prediction. \n",
    "### Potential improvments include: L1 regularization in both models for better feature selection, experimenting with different optimizers such as stochastic gradient descent, altered learning rate configuration, more layers for the MLP model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
